{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Report for Transaction-Receipt Matcher Task"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import cupy as cp # linear algebra\nimport cudf # data processing, CSV file I/O (e.g. cudf.read_csv)\nfrom cuml.preprocessing.model_selection import train_test_split\nfrom cuml.metrics import roc_auc_score, confusion_matrix, precision_recall_curve\nfrom cuml.preprocessing.LabelEncoder import LabelEncoder\n\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split as sktrain_test_split\nimport sklearn\nfrom random import shuffle\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport xgboost\nfrom cupy import asnumpy\n\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray import tune\nfrom ray.tune.integration.xgboost import TuneReportCheckpointCallback\n\n\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pprint\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Preview dataset** <a class=\"anchor\" id=\"4.3.2\"></a> "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = cudf.read_csv('/kaggle/input/tide-data/data_interview_test (1).csv',sep=\":\")\n\npdf = pd.read_csv('/kaggle/input/tide-data/data_interview_test (1).csv',sep=\":\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shape of dataset"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have total 12034 samples and each have mathcing feature vector of size of 10"},{"metadata":{},"cell_type":"markdown","source":"### Create labels \n- Transaction and Receipt are correct matches where matched_transaction_id = feature_transaction_id\n- We create a column for the labels using above condition"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data['label']= (data.matched_transaction_id == data.feature_transaction_id).astype(int)\npdf['label'] = (pdf.matched_transaction_id == pdf.feature_transaction_id).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check the distribution of label ( or correct vs wrong matches)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pdf.hist('label')\nprint(pdf.label.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The total samples in data are 12304 out of which 11177 are wrong matchings and only 857 correct match\n- Highly unbalanced data"},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Metric\n- Befor creating the models, we need to decide a evaluation metric.\n- Simple accuracy is bad metric here due to higly imbalanced data as randomly labeling as all incorrect matching the accuracy will be very high.\n- Recall = true positives/(true positives + false negatives) \n- Precison = true positives/(true positives + false positives)\n- recall is could be looked as ability of model to correctly label all true positives. and precison is ability to not label negative sample as positive.\n- Both of these are important but the major priority we will consider as to get a high Recall, so that correct matching could be found."},{"metadata":{},"cell_type":"markdown","source":"## Baseline\n- Create a baseline model using matching vector as feature.\n- Starting with simple linear regression and random forest, and xgboost model.\n- Xgboost model produces better result for baseline, so moving forward we will be optimizing the xgboost model only.\n"},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\ndata['label']= (data.matched_transaction_id == data.feature_transaction_id).astype(int)\nx,y = data[list(data.columns[4:])], data.label\nx=x.drop(['label'], axis=1)  \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42,shuffle=True)\n    \ntrain = xgboost.DMatrix(X_train, label=y_train)\ntest = xgboost.DMatrix(X_test, label=y_test)\nwatchlist = [(test, 'eval'), (train, 'train')]\n\nxgb_params = {\n            'objective': 'binary:logistic',\n            'tree_method': 'gpu_hist',\n            'max_depth': 2, \n            'eta':0.1,\n            'silent':1,\n            'subsample':0.5,\n            'colsample_bytree': 0.05,\n            \n}\n\nclf = xgboost.train( xgb_params,train, num_boost_round=10000,                 )\n\npreds = clf.predict(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(classification_report(asnumpy(y_test),preds.round(0)))\nprint( \" In the Confusion Matrix below, the digonal values represent correct classification for each class : \")\nlabels = ['label-0', 'label-1']\n#print(confusion_matrix((y_test),(preds.round(0).astype(int))))  \n\n\ncm = sklearn.metrics.confusion_matrix(asnumpy(y_test),asnumpy(preds.round(0).astype(int)))\n \n\nax= plt.subplot()\nsns.heatmap(cm.astype(int), annot=True,fmt='g', ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The baseline XGB produces decent results the precesion is high, but recall is low, meaning the model is missing lot of correct matches, which is not good.\n"},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\n- Now we dont have much inforamtion about how the features were created so it's hard to engineer any more features out of these, but looking at some of the features it could be seen all are values between 0-1 and we need to see how these value are related to final label.\n- Lets see the how distrubution of values of one of the feature column looks.\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pdf.hist(\"DateMappingMatch\")\nprint(\" Distribuition of DateMappingMatch feature values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It could be seen  DateMappingMatch is 0 for more than 8k samples.\n- We can check if this distribution is related to the label:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dg = data.groupby('DateMappingMatch').agg({'label':['mean']})\ndg.columns = ['label_mean']\nax = dg.label_mean.to_pandas().plot.bar()\nax.set_ylabel('label_mean')\n\nprint (\"DateMappingMatch related to average label value \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now it could be seen here that for DateMappingMatch values > 0.8 the corresponds to higher label mean, or larger DateMappingMatch value is realted to postive label.\n-  We can group these value in bins and count the size of each bin to create a new feature.\n- Using these new feature we train the model again."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\norg_data = data.copy()\nfeat_col=data.columns[4:-1]\nle = LabelEncoder()\nfor col in feat_col:\n    var_count = data.groupby(col).agg({col:'count'})\n    var_count.columns = ['%s_count'%col]\n    var_count = var_count.reset_index()\n    data = data.merge(var_count,on=col,how='left')\n    le.fit(data['%s_count'%col])  \n    encoded = le.transform(data['%s_count'%col])\n    data['%s_count'%col] = encoded/encoded.max()\n    \n\n\nx,y = data[list(data.columns[4:])], data.label\nx=x.drop(['label'], axis=1)  \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42,shuffle=True)\n    \ntrain = xgboost.DMatrix(X_train, label=y_train)\ntest = xgboost.DMatrix(X_test, label=y_test)\nwatchlist = [(test, 'eval'), (train, 'train')]\n\nxgb_params = {\n            'objective': 'binary:logistic',\n            'tree_method': 'gpu_hist',\n            'max_depth': 2, \n            'eta':0.1,\n            'silent':1,\n            'subsample':0.5,\n            'colsample_bytree': 0.05,\n            \n}\n\nclf = xgboost.train( xgb_params,train, num_boost_round=10000,                 )\n\npreds = clf.predict(test)\nprint(classification_report(asnumpy(y_test),preds.round(0)))\nprint( \" In the Confusion Matrix below, the digonal values represent correct classification for each class : \")\nlabels = ['label-0', 'label-1']\n#print(confusion_matrix((y_test),(preds.round(0).astype(int))))  \n\n\ncm = sklearn.metrics.confusion_matrix(asnumpy(y_test),asnumpy(preds.round(0).astype(int)))\n \n\nax= plt.subplot()\nsns.heatmap(cm.astype(int), annot=True,fmt='g', ax = ax);\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As seen from above results the new features doesn't make much diffrence\n- For now we won't use them as they don't provide any significant improvenment but will cause problems as creating these features during testing on new data will not be straight forward.\n- Also these feature are directly produced from distribution of training data, and this will be change in testing data, so these features are not robust as more data comes in future."},{"metadata":{},"cell_type":"markdown","source":"### We need to handle the class imbalance to improve the results.\n-  The best method would to have weighted loss, meaning if rare class is missclassifed the model would penalised more.\n- Now the weight by which we should be mupltiplying the loss could be taken as the imbalance proportion or any other value also might work, for now we could use 13 (which is class imbalance ratio)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndata = org_data\ndata['label']= (data.matched_transaction_id == data.feature_transaction_id).astype(int)\nx,y = data[list(data.columns[4:])], data.label\nx=x.drop(['label'], axis=1)  \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42,shuffle=True)\n    \ntrain = xgboost.DMatrix(X_train, label=y_train)\ntest = xgboost.DMatrix(X_test, label=y_test)\nwatchlist = [(test, 'eval'), (train, 'train')]\n\nxgb_params = {\n            'objective': 'binary:logistic',\n            'tree_method': 'gpu_hist',\n            'max_depth': 2, \n            'eta':0.1,\n            'silent':1,\n            'subsample':0.5,\n            'colsample_bytree': 0.05,\n            'scale_pos_weight':13,\n}\n\nclf = xgboost.train( xgb_params,train, num_boost_round=20000,                 )\n\npreds = clf.predict(test)\n\nprint(classification_report(asnumpy(y_test),preds.round(0)))\nprint( \" In the Confusion Matrix below, the digonal values represent correct classification for each class : \")\nlabels = ['label-0', 'label-1']\n#print(confusion_matrix((y_test),(preds.round(0).astype(int))))  \n\n\ncm = sklearn.metrics.confusion_matrix(asnumpy(y_test),asnumpy(preds.round(0).astype(int)))\n \n\nax= plt.subplot()\nsns.heatmap(cm.astype(int), annot=True,fmt='g', ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So this weighted loss trick improved the recall value significantly, although precesion for is not that high, but better a high recall value is more preferable as it corresponds to correctly finding the matching reciept.\n- Now as seen from Confusion Matrix, we can see we have more false postives (527) now although this is bad we shoudld check the confidence of model in producing these false postives."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tp = np.logical_and(preds.round(0)==1,asnumpy(y_test)==1)\nfp = np.logical_and(preds.round(0)==1,asnumpy(y_test)==0)\nxtest = X_test.copy()\nxtest['scores']=preds\nxtest.scores[fp].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The above stats signify that mean confidance of model classifying false postives is 0.72 also majority of false positivea have around 0.71 confidence, which is not good as model showing dignigicant confidance in wrong predection\n- We can check the model confidence for true postives (meaning correctly classifying the postive class) :"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"xtest.scores[tp].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - mean confidence for true postive is 0.88 which is good and higher than mean confidence in false postives, but it is still a problem, we somehow need to increase this diffrence without affecting recall value much.\n - Now the reason for this might be the we maybe using high weight(13) in the weighted loss, we can try to bring it down."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndata = org_data\ndata['label']= (data.matched_transaction_id == data.feature_transaction_id).astype(int)\nx,y = data[list(data.columns[4:])], data.label\nx=x.drop(['label'], axis=1)  \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42,shuffle=True)\n    \ntrain = xgboost.DMatrix(X_train, label=y_train)\ntest = xgboost.DMatrix(X_test, label=y_test)\nwatchlist = [(test, 'eval'), (train, 'train')]\n\nxgb_params = {\n            'objective': 'binary:logistic',\n            'tree_method': 'gpu_hist',\n            'max_depth': 2, \n            'eta':0.1,\n            'silent':1,\n            'subsample':0.5,\n            'colsample_bytree': 0.05,\n            'scale_pos_weight':8,\n}\n\nclf = xgboost.train( xgb_params,train, num_boost_round=20000,                 )\n\npreds = clf.predict(test)\n\nprint(classification_report(asnumpy(y_test),preds.round(0)))\nprint( \" In the Confusion Matrix below, the digonal values represent correct classification for each class : \")\nlabels = ['label-0', 'label-1']\n#print(confusion_matrix((y_test),(preds.round(0).astype(int))))  \n\n\ncm = sklearn.metrics.confusion_matrix(asnumpy(y_test),asnumpy(preds.round(0).astype(int)))\n \n\nax= plt.subplot()\nsns.heatmap(cm.astype(int), annot=True,fmt='g', ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tp = np.logical_and(preds.round(0)==1,asnumpy(y_test)==1)\nfp = np.logical_and(preds.round(0)==1,asnumpy(y_test)==0)\nxtest = X_test.copy()\nxtest['scores']=preds\nprint ( \"false postive stats\")\nprint(xtest.scores[fp].describe())\nprint()\nprint ( \"True postive stats\")\nprint(xtest.scores[tp].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Okay so we have brought the model confidence  in false postives lower and also diffrence of confidence between the 2 cases increased which is good with new weighted loss parameter as 8 instead of 13, and overall model is still not affected that much.\n - We are going in right direction.\n - Next step is to see if we can have better Hyperparameters for the model to further boost the performance.\n"},{"metadata":{},"cell_type":"markdown","source":"### HyperParameter optimization \n- now that we have very good model already, we can try to select best hyperparmeters for the xgb model we are using to further improve the performance.\n- We will be suing Ray Tune library for this purpose as it integrated nicely with the Cuml and Xgboost library we are using for faster trainging on GPU (compared to normal Scikit-Learn models on CPU).\n- Now after lot of trials setting diffrenet parameter search space, the resulting model is a huge improvenment over our previous best, also when the recall value is comaprable on cheking it is found that the model confidence is very low for both true psotives almsot same as for false positives which is not correct. \n- We can try different Hyperparameter search algorithms but the major problem comes in setting right metric for the search as simple accuracy or loss or AUC doesn't produce good result for our aim for matching right recipt with good confidence, also it's very time consuming and it very often produce overfitted model due to amount of data.\n- So we will won't use the model from the hyperparameter search results and go out with intial choosen model.\n\nResults from HyperParameter Search :"},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip uninstall -y dataclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndef prep_data(path):\n    data = cudf.read_csv(path,sep=\":\")\n    data['label']= (data.matched_transaction_id == data.feature_transaction_id).astype(int)\n    feat_col=data.columns[4:-1]\n    le = LabelEncoder()\n    for col in feat_col:\n        var_count = data.groupby(col).agg({col:'count'})\n        var_count.columns = ['%s_count'%col]\n        var_count = var_count.reset_index()\n        data = data.merge(var_count,on=col,how='left')\n        le.fit(data['%s_count'%col])  \n        encoded = le.transform(data['%s_count'%col])\n        data['%s_count'%col] = encoded/encoded.max()\n    x,y = data[list(data.columns[4:])], data.label\n    x=x.drop(['label'], axis=1)  \n    return x,y\npath = \"/kaggle/input/tide-data/data_interview_test (1).csv\"\nx,y = prep_data(path)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42,shuffle=True)\n\ndef train_receipt_match(config):\n       \n    train = xgboost.DMatrix(X_train, label=y_train)\n    test = xgboost.DMatrix(X_test, label=y_test)\n    watchlist = [(test, 'eval'), (train, 'train')]\n    clf = xgboost.train(config, train, num_boost_round=10000,\n                        evals=watchlist,\n                        maximize=True,\n                        verbose_eval=1000,\n                        callbacks=[TuneReportCheckpointCallback(filename=\"model.xgb\")]\n                       )\n    \n\nsearch_space = {\n    # You can mix constants with search space objects.\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": [\"logloss\", \"error\", \"aucpr\"],\n    \"max_depth\": tune.randint(1, 9),\n    \"min_child_weight\": tune.choice([1, 2, 3]),\n    \"subsample\": tune.uniform(0.4, 1.0),\n    \"eta\": tune.loguniform(1e-4, 1e-1),\n    \"scale_pos_weight\":tune.randint(3, 13),\n}\n\n# This will enable aggressive early stopping of bad trials.\nscheduler = ASHAScheduler(\n    max_t=10,  # 10 training iterations\n    grace_period=1,\n    reduction_factor=2)\n\nanalysis = tune.run(\n    train_receipt_match,\n    metric=\"eval-aucpr\",\n    mode=\"max\",\n    # You can add \"gpu\": 0.1 to allocate GPUs\n    resources_per_trial={\"gpu\": 1},\n    config=search_space,\n    num_samples=10,\n    scheduler=scheduler)\n\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As seen below sometime the resulting model from hyper-parameter search performs lesser than our previous best.\n- Also in some experiments hyper parameter search produces overfitted model as it usualy produces model with high depth of trees which make Xgboost model more complex and subject to overfitting."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"best_bst = xgboost.Booster()\nbest_bst.load_model(os.path.join(analysis.best_checkpoint, \"model.xgb\"))\naucpr = analysis.best_result[\"eval-aucpr\"]\npprint.pprint(f\"Best model parameters: {analysis.best_config}\")\nprint(f\"Best model total eval-aucpr: {aucpr}\")\n\npreds = best_bst.predict(test)\n\nprint(classification_report(asnumpy(y_test),preds.round(0)))\nprint( \" In the Confusion Matrix below, the digonal values represent correct classification for each class : \")\nlabels = ['label-0', 'label-1']\n#print(confusion_matrix((y_test),(preds.round(0).astype(int))))  \n\n\ncm = sklearn.metrics.confusion_matrix(asnumpy(y_test),asnumpy(preds.round(0).astype(int)))\n \n\nax= plt.subplot()\nsns.heatmap(cm.astype(int), annot=True,fmt='g', ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\n- We were able to make good classifier which is able to match the correct ‘transaction-receipt’ with high confidence.\n- Selecting the right metric was hard as all the usual metrics comes with some issues, and we need to further analyse the model to check how confident the model is in making right or wrong decision even it produce results on choosen metrics.\n- Hyper-Parameter search is hard for such kind of imbalanced data and low number of samples.\n- Due to limited information on current feature vector new feature engineering is difficult.\n\n- Creating an ensemble of models would have resulted in little better performance but it's hard to deploy, so did't created any ensemble of multiple models.\n- We did our complete modeling using Libraries(Cuml, Cudf) from RapidsAI which enables us to use ML algorithms on GPU's.\n- Also we can port the model directly to be used on CPU without any extra step and we dont need to use these libs during inference."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}